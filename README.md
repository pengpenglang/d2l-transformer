- ç”¨äºBNUã€Šæ•°æ®ç§‘å­¦å¯¼è®ºã€‹è¯¾ç¨‹å±•ç¤º
- ä»£ç æ¥æºäºææ²çš„[ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ï¼ˆç¬¬äºŒç‰ˆï¼‰ã€‹ç« èŠ‚10.7 Transformer](https://www.google.com/url?q=https%3A%2F%2Fzh.d2l.ai%2Fchapter_attention-mechanisms%2Ftransformer.html)
- æ³¨é‡Šä¿®æ”¹è‡ªpengpenglangï¼Œä»…ä¾›å‚è€ƒï¼Œéœ€è¦åœ¨é…æœ‰NVIDIA GPUçš„è®¡ç®—æœºä¸­è¿è¡Œã€‚d2læ˜¯ä¸€ä¸ªåŸºäºPythonçš„æ·±åº¦å­¦ä¹ å¹³å°ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„æ·±åº¦å­¦ä¹ å·¥å…·å’Œèµ„æºï¼ŒåŒ…æ‹¬å„ç§ç¥ç»ç½‘ç»œæ¨¡å‹ã€æ•°æ®é›†ã€å¯è§†åŒ–å·¥å…·ç­‰

> å»ºè®®åŒå­¦ä»¬å€ŸåŠ©è°·æ­Œcolabå¹³å°åœ¨çº¿è¿è¡Œï¼ŒğŸ‘‰[å®˜ç½‘ä»£ç ](https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_attention-mechanisms-and-transformers/transformer.ipynb)ï¼ŒğŸ‘‰[è¯¾ä¸Šå±•ç¤ºä»£ç ](https://colab.research.google.com/github/pengpenglang/d2l-transformer/blob/main/chapter_attention-mechanisms/transformer.ipynb)